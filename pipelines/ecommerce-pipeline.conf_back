# Configuration Logstash pour l'ingestion de logs e-commerce
# Fichier: /etc/logstash/conf.d/ecommerce-pipeline.conf

input {
  # Lecture des fichiers de logs avec rotation
  file {
    path => "/var/log/ecommerce/access.log*"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_ecommerce"
    codec => "plain"
    type => "ecommerce_access"
  }
  
  # Lecture depuis Kafka pour les données en temps réel
  kafka {
    bootstrap_servers => "localhost:9092"
    topics => ["ecommerce-events"]
    group_id => "logstash-ecommerce"
    codec => "json"
    type => "ecommerce_events"
  }
  
  # Lecture depuis base de données (batch processing)
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/ecommerce"
    jdbc_user => "logstash"
    jdbc_password => "password"
    jdbc_driver_library => "/usr/share/logstash/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    statement => "SELECT * FROM orders WHERE updated_at > :sql_last_value"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "*/5 * * * *"
    type => "ecommerce_orders"
  }
}

filter {
  # Traitement des logs d'accès
  if [type] == "ecommerce_access" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG} %{GREEDYDATA:extra_fields}"
      }
    }
    
    # Parsing de l'user agent
    useragent {
      source => "agent"
      target => "user_agent"
    }
    
    # Extraction des informations de l'URL
    if [request] {
      grok {
        match => { 
          "request" => "(?<method>\w+) (?<url>[^?]*)\??(?<query_params>[^?]*) HTTP/(?<http_version>[\d\.]+)"
        }
      }
    }
    
    # Géolocalisation IP
    geoip {
      source => "clientip"
      target => "geoip"
      database => "/etc/logstash/GeoLite2-City.mmdb"
    }
    
    # Classification du device
    if [user_agent][device] {
      if [user_agent][device] == "Other" {
        mutate {
          add_field => { "device_category" => "desktop" }
        }
      } else {
        mutate {
          add_field => { "device_category" => "mobile" }
        }
      }
    }
    
    # Conversion des types
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
    
    # Catégorisation du statut HTTP
    if [response] >= 200 and [response] < 300 {
      mutate { add_field => { "status_category" => "success" } }
    } else if [response] >= 300 and [response] < 400 {
      mutate { add_field => { "status_category" => "redirect" } }
    } else if [response] >= 400 and [response] < 500 {
      mutate { add_field => { "status_category" => "client_error" } }
    } else if [response] >= 500 {
      mutate { add_field => { "status_category" => "server_error" } }
    }
  }
  
  # Traitement des événements Kafka
  if [type] == "ecommerce_events" {
    # Enrichissement avec données de session
    if [session_id] {
      elasticsearch {
        hosts => ["localhost:9200"]
        index => "sessions"
        query => "session_id:%{session_id}"
        fields => { "user_id" => "user_id", "session_start" => "session_start" }
      }
    }
    
    # Calcul de la durée de session
    if [session_start] {
      ruby {
        code => "
          session_start = Time.parse(event.get('session_start'))
          current_time = Time.parse(event.get('@timestamp'))
          event.set('session_duration', current_time - session_start)
        "
      }
    }
    
    # Enrichissement produit
    if [product_id] {
      elasticsearch {
        hosts => ["localhost:9200"]
        index => "products"
        query => "product_id:%{product_id}"
        fields => { 
          "category" => "product_category",
          "price" => "product_price",
          "brand" => "product_brand"
        }
      }
    }
  }
  
  # Traitement des commandes
  if [type] == "ecommerce_orders" {
    # Calcul des métriques
    ruby {
      code => "
        items = event.get('order_items')
        if items
          total_items = items.length
          total_value = items.sum { |item| item['price'] * item['quantity'] }
          event.set('total_items', total_items)
          event.set('total_value', total_value)
        end
      "
    }
    
    # Classification des commandes
    if [total_value] {
      if [total_value] > 500 {
        mutate { add_field => { "order_category" => "high_value" } }
      } else if [total_value] > 100 {
        mutate { add_field => { "order_category" => "medium_value" } }
      } else {
        mutate { add_field => { "order_category" => "low_value" } }
      }
    }
  }
  
  # Filtres communs
  # Suppression des champs non nécessaires
  mutate {
    remove_field => ["message", "host", "path"]
  }
  
  # Ajout d'un timestamp normalisé
  date {
    match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
    target => "@timestamp"
  }
  
  # Détection d'anomalies (exemple simple)
  if [clientip] {
    aggregate {
      task_id => "%{clientip}"
      code => "
        map['requests'] ||= 0
        map['requests'] += 1
        if map['requests'] > 1000
          event.set('anomaly', 'high_request_rate')
        end
      "
      push_map_as_event_on_timeout => true
      timeout => 60
    }
  }
}

output {
  # Sortie vers Elasticsearch avec des index par type et date
  if [type] == "ecommerce_access" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "ecommerce-access-%{+YYYY.MM.dd}"
      template_name => "ecommerce-access"
      template => "/etc/logstash/templates/ecommerce-access-template.json"
      template_overwrite => true
      routing => "%{clientip}"
    }
  }
  
  if [type] == "ecommerce_events" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "ecommerce-events-%{+YYYY.MM.dd}"
      template_name => "ecommerce-events"
      template => "/etc/logstash/templates/ecommerce-events-template.json"
      template_overwrite => true
    }
  }
  
  if [type] == "ecommerce_orders" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "ecommerce-orders-%{+YYYY.MM}"
      template_name => "ecommerce-orders"
      template => "/etc/logstash/templates/ecommerce-orders-template.json"
      template_overwrite => true
    }
  }
  
  # Sortie vers Kafka pour alertes
  if [anomaly] {
    kafka {
      bootstrap_servers => "localhost:9092"
      topic_id => "alerts"
      codec => "json"
    }
  }
  
  # Debug output (à désactiver en production)
  stdout { codec => rubydebug }
}
